{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Tagging for Customer Churn Demo\n",
    "We can use Spark Execution Engine to do the feature engineering and tagging for both train data and score data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Notebook setup\n",
    "\n",
    "When using Spark kernel notebooks on HDInsight, there is no need to create a SparkContext or a HiveContext; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkContext (sc)\n",
    "- HiveContext (sqlContext)\n",
    "\n",
    "To run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "## Variables set up\n",
    "\n",
    "We need to set up three variables:\n",
    "\n",
    "ChurnPeroid: This is the period you want to set to define the customer churn. Default value is 21 days. \n",
    "\n",
    "ChurnThreshold: This is the threshold you want to set to define the customer churn. The threshold defines as the number transactions a customer has at the churnPeriod. Default value is 0,  which means a customer churned if he/she doesnâ€™t have any transaction during the churnPeriod.\n",
    "\n",
    "DataDir: This is the storage path, please replace `$datacontainer` and `$storagename` with the real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val churnPeriodVal=21;\n",
    "val churnThresholdVal=0;\n",
    "val dataDir=\"wasb://$datacontainer@$storagename.blob.core.windows.net;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We create a dataframe on these two variables in order to to joins later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val ChurnVarsDF = sqlContext.createDataFrame(Seq((churnPeriodVal, churnThresholdVal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Reading data from Hive\n",
    "\n",
    "To start with, let's first see what we have in our Hive store. The database in hive for the demo is `customerchurn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "use customerchurn;\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create dataframe for user transaction activities using HIVE partitioned table `activities`, The snippet below creates a dataframe that you can perform any dataframe operation on. This dataframe contains all the data in the `activities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val activityTableDF = sqlContext.sql(\"select * from customerchurn.activities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the max Datetime for user transaction activities and assign it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val maxTimeDF = activityTableDF.select(max($\"TransactionTime\").alias(\"maxAllTransDate\"))\n",
    "val maxAllTransDateVal = (maxTimeDF.rdd.first())(0).toString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create dataframe for user demograph data using HIVE partitioned table `users`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val userTableDF = sqlContext.sql(\"select * from customerchurn.users\").join(ChurnVarsDF.withColumnRenamed(\"_1\", \"ChurnPeriod\").withColumnRenamed(\"_2\", \"ChurnThreshold\")).join(maxTimeDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Analytic Functions to get lag of the TransactionTime for each user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val w = Window.partitionBy(\"UserId\").orderBy(\"TransactionTime\")\n",
    "val activityLagTableDF = activityTableDF.select($\"*\", datediff($\"TransactionTime\", lag($\"TransactionTime\", 1).over(w)).alias(\"TransactionInterval\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "produce a Pre-Churn flag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val exprStr = \"case when datediff(TransactionTime, date_add('\" + maxAllTransDateVal + \"', -1*\" + churnPeriodVal + \")) <= 0 then 1 else 0 end\"\n",
    "val activityFlagTableDF = activityLagTableDF.withColumn(\"preChurnPeriodTransFlag\", expr(exprStr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce all the features by join two dataframes \n",
    "Tag as Churn or Non-Churn for train data. For score data, we also tag it for comparison purpose with Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val featuredDF = (\n",
    "         activityFlagTableDF\n",
    "         .groupBy($\"UserId\")\n",
    "         .agg(  sum(expr(\"case when preChurnPeriodTransFlag = 1 then 1 else 0 end\")).alias(\"PrechurnProductsPurchased\"), \n",
    "                count($\"TransactionId\").alias(\"OverallProductsPurchased\"),\n",
    "                sum(expr(\"case when preChurnPeriodTransFlag = 1 then Quantity else 0 end\")).alias(\"TotalQuantity\"), \n",
    "                sum(expr(\"case when preChurnPeriodTransFlag = 1 then Value else 0 end\")).alias(\"TotalValue\"),\n",
    "                stddev_samp(expr(\"case when preChurnPeriodTransFlag = 1 then Quantity else null end\")).alias(\"StDevQuantity\"), \n",
    "                stddev_samp(expr(\"case when preChurnPeriodTransFlag = 1 then Value else null end\")).alias(\"StDevValue\"),             \n",
    "                avg(expr(\"case when preChurnPeriodTransFlag = 1 then TransactionInterval else null end\")).alias(\"AvgTimeDelta\"),\n",
    "                (max(expr(\"case when preChurnPeriodTransFlag = 1 then TransactionTime else null end\"))).alias(\"RecencyDate\"),\n",
    "                (countDistinct(expr(\"case when preChurnPeriodTransFlag = 1 then TransactionId else '-1' end\")) \n",
    "                 - sumDistinct(expr(\"case when (case when preChurnPeriodTransFlag = 1 then TransactionId else null end) is null then 1 else 0 end\"))).alias(\"UniqueTransactionId\"),\n",
    "                (countDistinct(expr(\"case when preChurnPeriodTransFlag = 1 then ItemId else '-1' end\")) \n",
    "                 - sumDistinct(expr(\"case when (case when preChurnPeriodTransFlag = 1 then ItemId else null end) is null then 1 else 0 end\"))).alias(\"UniqueItemId\"),\n",
    "                (countDistinct(expr(\"case when preChurnPeriodTransFlag = 1 then Location else '-1' end\")) \n",
    "                 - sumDistinct(expr(\"case when (case when preChurnPeriodTransFlag = 1 then Location else null end) is null then 1 else 0 end\"))).alias(\"UniqueLocation\"),\n",
    "                (countDistinct(expr(\"case when preChurnPeriodTransFlag = 1 then ProductCategory else '-1' end\")) \n",
    "                 - sumDistinct(expr(\"case when (case when preChurnPeriodTransFlag = 1 then ProductCategory else null end) is null then 1 else 0 end\"))).alias(\"UniqueProductCategory\")\n",
    "          )\n",
    "          .join(userTableDF.withColumnRenamed(\"UserID\", \"UId\"), $\"UId\"===activityFlagTableDF(\"UserId\"))\n",
    "          .select($\"UserId\", \n",
    "          $\"TotalQuantity\", \n",
    "          $\"TotalValue\", \n",
    "          $\"StDevQuantity\", \n",
    "          $\"StDevValue\", \n",
    "          $\"AvgTimeDelta\", \n",
    "                   (datediff($\"maxAllTransDate\", $\"RecencyDate\") - $\"ChurnPeriod\").alias(\"Recency\"), \n",
    "                   $\"UniqueTransactionId\", $\"UniqueItemId\", $\"UniqueLocation\", $\"UniqueProductCategory\", \n",
    "                   ($\"TotalQuantity\" /($\"UniqueTransactionId\"+1)).alias(\"TotalQuantityperUniqueTransactionId\"), \n",
    "                   ($\"TotalQuantity\" /($\"UniqueItemId\"+1)).alias(\"TotalQuantityperUniqueItemId\"), \n",
    "                   ($\"TotalQuantity\" /($\"UniqueLocation\"+1)).alias(\"TotalQuantityperUniqueLocation\"), \n",
    "                   ($\"TotalQuantity\" /($\"UniqueProductCategory\"+1)).alias(\"TotalQuantityperUniqueProductCategory\"), \n",
    "                   ($\"TotalValue\" /($\"UniqueTransactionId\"+1)).alias(\"TotalValueperUniqueTransactionId\"), \n",
    "                   ($\"TotalValue\" /($\"UniqueItemId\"+1)).alias(\"TotalValueperUniqueItemId\"), \n",
    "                   ($\"TotalValue\" /($\"UniqueLocation\"+1)).alias(\"TotalValueperUniqueLocation\"), \n",
    "                   ($\"TotalValue\" /($\"UniqueProductCategory\"+1)).alias(\"TotalValueperUniqueProductCategory\"),\n",
    "                   $\"Age\",\n",
    "                   $\"Address\",\n",
    "                   $\"Gender\",\n",
    "                   $\"UserType\",\n",
    "                    expr(\"case when PrechurnProductsPurchased = 0 then 0 when PrechurnProductsPurchased >=0 and (( OverallProductsPurchased- PrechurnProductsPurchased)<= ChurnThreshold)  then 1 else 0 end\").alias(\"churn\"),\n",
    "                    $\"PrechurnProductsPurchased\",\n",
    "                    $\"OverallProductsPurchased\"                   \n",
    "                  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "## Save the featured/tagged data back to storage\n",
    "\n",
    "First prepare the blob. Because for Hive External table, Scala write.mode(SaveMode.Overwrite) could not produce the data properly,  \n",
    "Because we have to use write.mode(SaveMode.Overwrite), we have to remove the old data may pre-existing there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val filePath= dataDir + \"/customerchurn/data/traindatauserfeatured/\"\n",
    "Seq(\"hadoop\",\"fs\",\"-mkdir\", \"-p\",filePath).!!    \n",
    "Seq(\"hadoop\",\"fs\",\"-rm\", \"-r\",filePath).!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a dataframe that was created with a HiveContext and you want to persist that data to Hive, you can create a table and then insert the dataframe into the table: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"use customerchurn\")\n",
    "sqlContext.sql(\"drop table traindata_user_Featured\")\n",
    "\n",
    "val sqlStr = \"\"\"\n",
    "    CREATE EXTERNAL TABLE traindata_user_Featured(\n",
    "        UserId varchar(50) ,\n",
    "        TotalQuantity bigint ,\n",
    "        TotalValue float ,\n",
    "        StDevQuantity float ,\n",
    "        StDevValue float ,\n",
    "        AvgTimeDelta float ,\n",
    "        Recency int ,\n",
    "        UniqueTransactionId bigint ,\n",
    "        UniqueItemId bigint ,\n",
    "        UniqueLocation bigint ,\n",
    "        UniqueProductCategory bigint ,\n",
    "        TotalQuantityperUniqueTransactionId float ,\n",
    "        TotalQuantityperUniqueItemId float ,\n",
    "        TotalQuantityperUniqueLocation float ,\n",
    "        TotalQuantityperUniqueProductCategory float ,\n",
    "        TotalValueperUniqueTransactionId float ,\n",
    "        TotalValueperUniqueItemId float ,\n",
    "        TotalValueperUniqueLocation float ,\n",
    "        TotalValueperUniqueProductCategory float ,\n",
    "        Age varchar(50) ,\n",
    "        Address varchar(50) ,\n",
    "        Gender varchar(50),\n",
    "        UserType varchar(50),\n",
    "        tag   varchar(10),\n",
    "        PrechurnProductsPurchased bigint ,\n",
    "        OverallProductsPurchased bigint \n",
    "    )\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "    LINES TERMINATED BY '10' \n",
    "    STORED AS TEXTFILE LOCATION \n",
    "    \"\"\" + \"'\" + dataDir + \"/customerchurn/data/traindatauserfeatured/'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose 70% data as train data, and then save as hive table to blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuredDF.sample(false, 0.7, 123).coalesce(1).write.mode(SaveMode.Append).saveAsTable(\"traindata_user_Featured\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete the log files produced by Hive, because MRS could not recongized them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val lsFilePath= (Seq(\"hadoop\",\"fs\",\"-ls\",filePath).!!).replace(\"\\n\", \" \")\n",
    "val tempFileList= lsFilePath.split(\" \").filter(x => (x.contains(\".hive-staging_hive\")))\n",
    "    \n",
    "\n",
    "for(tempFilePath<- tempFileList)\n",
    "    {\n",
    "       Seq(\"hadoop\",\"fs\",\"-rm\", \"-r\",tempFilePath).!!\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}